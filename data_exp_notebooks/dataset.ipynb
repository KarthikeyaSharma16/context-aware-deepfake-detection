{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from transformers import BertTokenizer\n",
    "from torchvision.transforms import Compose, Lambda, Resize, Normalize, ColorJitter\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, video_path_file, text_csv_file, text_transforms=None, video_transforms=None, num_frames=8, sampling_rate=8, frames_per_second=30):\n",
    "        self.video_annotation = pd.read_csv(video_path_file)\n",
    "        self.text_df = pd.read_csv(text_csv_file)\n",
    "        self.text_transforms = text_transforms\n",
    "        self.video_transforms = video_transforms\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_annotation)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        video_path = self.video_annotation.iloc[index]['video_path']\n",
    "        label = self.video_annotation.iloc[index]['label']\n",
    "        text = self.text_df.iloc[index]['text']\n",
    "\n",
    "        try:\n",
    "            # Load video using PyTorchVideo\n",
    "            video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "            # Get video duration and calculate the step size for frame sampling\n",
    "            duration = video.duration\n",
    "            step = duration / self.num_frames\n",
    "            print(f'Video length: {duration}')\n",
    "\n",
    "            # Sample frames at regular intervals\n",
    "            video_data = []\n",
    "            for i in range(self.num_frames):\n",
    "                start_sec = i * step\n",
    "                end_sec = start_sec + step\n",
    "                clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "                print(f'clip shape: {clip['video'].shape}')\n",
    "                transformed_clip = self.video_transforms(clip['video'])\n",
    "                print(f'Transformed Clip: {transformed_clip.shape}')\n",
    "                video_data.append(transformed_clip)\n",
    "\n",
    "            # Stack the sampled frames\n",
    "            video_data = torch.stack(video_data)\n",
    "            # print(video_data)\n",
    "            print(f'Video_data shape: {video_data.shape}')\n",
    "            print('--------------------------------------------\\n')\n",
    " \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error Processing video {video_path}: {e}')\n",
    "            # print(f'Clip Duration: {clip_duration}')\n",
    "            # print(f'Video Duration: {video.duration}')\n",
    "\n",
    "        # Apply text transforms\n",
    "        if self.text_transforms:\n",
    "            text_data = self.text_transforms(text)\n",
    "        else:\n",
    "            text_data = text\n",
    "\n",
    "        return {\n",
    "            'video': video_data,\n",
    "            'text': text_data,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of text and video transforms\n",
    "text_transforms = Compose([\n",
    "    BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    # Add more text transformations as needed\n",
    "])\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 30\n",
    "\n",
    "\n",
    "video_transforms = Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(1),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=side_size),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Create an instance of the dataset\n",
    "video_path_file = '../annotations/video_train_path.csv'\n",
    "text_csv_file = '../annotations/text_train.csv'\n",
    "\n",
    "dataset = DeepFakeDataset(\n",
    "                            video_path_file=video_path_file,\n",
    "                            text_csv_file=text_csv_file,\n",
    "                            text_transforms=text_transforms,\n",
    "                            video_transforms=video_transforms,\n",
    "                            num_frames=num_frames\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "batch_size = 8\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
